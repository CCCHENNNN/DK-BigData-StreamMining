{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Loading training data... 78 sequences, 147953 tokens.\n",
      "Loading test data... 19 sequences, 34529 tokens.\n",
      "78 sequences, 147953 tokens.\n",
      "[2283  757 3129  777 2759 2283  821 1518 1402  633  829 3221 1648  249\n",
      " 2014  587 1429 2079 1983  931 2845 2033 2116 2079 2158  994 2069 1849\n",
      " 2788 1892 2790 2024 2721 2469 1917 2715 1587 1743 2551 1611  859 1951\n",
      " 2964 2065 2156 1945 1898 2107 2094 2183 2039 2964 2142 1846 1847 2076\n",
      " 2112 1946 1461 2141 1650  620 3124 1483 2594 2612 2239 1450  849 1881\n",
      " 2293 1745 2212 2213 1795  757  463 2894]\n",
      "Training <hmm.HMM object at 0x1a1857c048>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../hmm.py:59: RuntimeWarning: invalid value encountered in log\n",
      "  feature_prob = np.log(safe_sparse_dot(Y.T, X) + rate)\n",
      "/Users/ericstarrk/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n",
      "Accuracy: 2.346\n",
      "CoNLL F1: 4.098\n"
     ]
    }
   ],
   "source": [
    "import fileinput\n",
    "from glob import glob\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from seqlearn.datasets import load_conll\n",
    "from seqlearn.evaluation import bio_f_score\n",
    "from hmm import HMM\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def features(sentence, i):\n",
    "    \"\"\"Features for i'th token in sentence.\n",
    "    Currently baseline named-entity recognition features, but these can\n",
    "    easily be changed to do POS tagging or chunking.\n",
    "    \"\"\"\n",
    "\n",
    "    word = sentence[i]\n",
    "\n",
    "    yield \"word:{}\" + word.lower()\n",
    "\n",
    "    if word[0].isupper():\n",
    "        yield \"CAP\"\n",
    "\n",
    "    if i > 0:\n",
    "        yield \"word-1:{}\" + sentence[i - 1].lower()\n",
    "        if i > 1:\n",
    "            yield \"word-2:{}\" + sentence[i - 2].lower()\n",
    "    if i + 1 < len(sentence):\n",
    "        yield \"word+1:{}\" + sentence[i + 1].lower()\n",
    "        if i + 2 < len(sentence):\n",
    "            yield \"word+2:{}\" + sentence[i + 2].lower()\n",
    "\n",
    "\n",
    "def describe(X, lengths):\n",
    "    print(\"{0} sequences, {1} tokens.\".format(len(lengths), X.shape[0]))\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    files = glob('nerdata/*.bio')\n",
    "\n",
    "    # 80% training, 20% test\n",
    "    print(\"Loading training data...\", end=\" \")\n",
    "    train_files = [f for i, f in enumerate(files) if i % 5 != 0]\n",
    "    train = load_conll(fileinput.input(train_files), features)\n",
    "    X_train, _, lengths_train = train\n",
    "    describe(X_train, lengths_train)\n",
    "\n",
    "    print(\"Loading test data...\", end=\" \")\n",
    "    test_files = [f for i, f in enumerate(files) if i % 5 == 0]\n",
    "    test = load_conll(fileinput.input(test_files), features)\n",
    "    X_test, _, lengths_test = test\n",
    "    describe(X_test, lengths_test)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(__doc__)\n",
    "\n",
    "    #print(\"Loading training data...\", end=\" \")\n",
    "    #X_train, y_train, lengths_train = load_conll(sys.argv[1], features)\n",
    "    #describe(X_train, lengths_train)\n",
    "\n",
    "    train, test = load_data()\n",
    "    X_train, y_train, lengths_train = train\n",
    "    X_test, y_test, lengths_test = test\n",
    "\n",
    "    #print(\"Loading test data...\", end=\" \")\n",
    "    #X_test, y_test, lengths_test = load_conll(sys.argv[2], features)\n",
    "    #describe(X_test, lengths_test)\n",
    "    describe(X_train, lengths_train)\n",
    "    print(lengths_train)\n",
    "    clf = HMM()\n",
    "    print(\"Training %s\" % clf)\n",
    "    sum = 0\n",
    "    for i in lengths_train:\n",
    "        clf.fit(X_train[0:sum+i], y_train[0:sum+i])\n",
    "        sum = sum + i\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print(\"Accuracy: %.3f\" % (100 * accuracy_score(y_test, y_pred)))\n",
    "        print(\"CoNLL F1: %.3f\" % (100 * bio_f_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7561030741410488\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from hmm import HMM\n",
    "# from hmm_stream import HMMStream\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "def getData():\n",
    "    data = pd.read_csv(\"words.csv\")\n",
    "    alphabet_dict = dict(zip(string.ascii_lowercase, range(1,27)))\n",
    "    reverse_dict = dict(zip(range(1,27), string.ascii_lowercase))\n",
    "    X = []\n",
    "    Y = []\n",
    "    for index, row in data.iterrows():\n",
    "        w_class = 4\n",
    "        singular = row[0]\n",
    "        plural = row[1]\n",
    "        singular_without_end = singular[:len(singular)-1]\n",
    "        #append -es cases\n",
    "        if (plural == singular + 'es'):\n",
    "            w_class = 2\n",
    "        #simply append -s cases\n",
    "        elif (plural == singular + 's'):\n",
    "            w_class = 1\n",
    "        #endings with -y\n",
    "        elif ((len(singular) > 1) and (plural == singular_without_end + 'ies')): # y -> ies\n",
    "            w_class = 3\n",
    "        elif (plural == singular):\n",
    "            w_class = 0\n",
    "        if (w_class != 4):\n",
    "            word_int = []\n",
    "            plural_int = []\n",
    "            for let in singular:\n",
    "                new_let = [0] * 26\n",
    "                new_let[alphabet_dict[let] - 1] = 1\n",
    "                word_int.extend(new_let)\n",
    "            X.append(word_int)\n",
    "            Y.append(w_class)\n",
    "\n",
    "    padded_X = np.zeros([len(X), len(max(X, key = lambda x: len(x)))])\n",
    "    for i, j in enumerate(X):\n",
    "        padded_X[i][0:len(j)] = j\n",
    "    words_int = np.column_stack((padded_X, Y))\n",
    "    pd.DataFrame(words_int).to_csv(\"words_int.csv\",index=False,sep=',')\n",
    "    return padded_X, Y\n",
    "\n",
    "padded_X, Y = getData()\n",
    "train_X = padded_X[:10000]\n",
    "train_Y = Y[:10000]\n",
    "test_X = padded_X[10000:]\n",
    "test_Y = Y[10000:]\n",
    "clf = HMM()\n",
    "clf.fit(train_X, train_Y)\n",
    "accuracy = clf.score(test_X, test_Y)\n",
    "print('accuracy: ', accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 121)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m121\u001b[0m\n\u001b[0;31m    def partial_fit(self, X, y=None, classes=None):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.misc import logsumexp\n",
    "from sklearn.externals import six\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "\n",
    "from util import atleast2d_or_csr, count_trans, validate_lengths\n",
    "\n",
    "from skmultiflow.core.base import StreamModel\n",
    "from skmultiflow.utils.data_structures import InstanceWindow\n",
    "from hmm import HMM\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "class HMMStream():\n",
    "    def __init__(self, window_size=100, max_models=100, delay=1):\n",
    "        self.H = []\n",
    "        self.h = None\n",
    "        # TODO\n",
    "        \n",
    "        self.counter = 0\n",
    "        self.window_size = window_size\n",
    "        self.max_models = max_models\n",
    "        self.window = InstanceWindow(window_size)\n",
    "        self.delay = delay\n",
    "        self.delay_counter = 0\n",
    "    def fit(self, X, y = None):\n",
    "        self.h = HMM()\n",
    "        self.h.fit(X, y)\n",
    "        self.H.append(self.h)\n",
    "        return self\n",
    "    def partial_fit(self, X, y = None, classes=None):\n",
    "        ''' partial_fit\n",
    "        \n",
    "        Update the HMM with new X, y\n",
    "\n",
    "                Parameters\n",
    "        ----------\n",
    "        X: Array-like\n",
    "            The feature's matrix.\n",
    "\n",
    "        y: Array-like\n",
    "            The class labels for all samples in X.\n",
    "\n",
    "        classes: list, optional\n",
    "            A list with all the possible labels of the classification problem.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        HMM\n",
    "            self\n",
    "\n",
    "        '''\n",
    "        print(\"start fit\")\n",
    "        N, D = X.shape;\n",
    "        \n",
    "        for i in range(N):\n",
    "            \n",
    "            self.window.add_element(np.asarray([X[i]]), np.asarray([[y[i]]]))\n",
    "            self.counter += 1\n",
    "            if (self.h) is None:\n",
    "                self.h = DecisionTreeClassifier()\n",
    "\n",
    "            # fit the new model if window_size is sufficient\n",
    "            if self.counter == self.window_size:\n",
    "                X_iter = self.window.get_attributes_matrix()\n",
    "                y_iter = self.window.get_targets_matrix()\n",
    "                self.h.fit(X_iter, y_iter)\n",
    "                self.counter=0\n",
    "                \n",
    "                # remove the most ancient model if the model_size is over than max_models\n",
    "                if(len(self.H) == self.max_models):\n",
    "                    self.H.pop(0)\n",
    "                self.H.append(self.h)\n",
    "        print(\"end fit\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" predict\n",
    "        \n",
    "        Predict expected y of given X        \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: Array like\n",
    "            The feature's matrix.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            A list containing the predicted labels for all instances in X.\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"start predict\")\n",
    "        N, D = X.shape\n",
    "        res = zeros(len(self.H)) \n",
    "        \n",
    "        print(\"len of H:\",len(self.H))\n",
    "        for i in range(len(res)):\n",
    "            res[i] = self.H[i].predict(X)\n",
    "        print(\"end predict\")\n",
    "        return res\n",
    "    def get_info(self):\n",
    "        return 'Hidden Markov Model Streamming'\n",
    "\n",
    "class BatchClassifier:\n",
    "\n",
    "    def __init__(self, window_size=100, max_models=100, delay=1):\n",
    "        self.H = []\n",
    "        self.h = None\n",
    "        # TODO\n",
    "        \n",
    "        self.counter = 0\n",
    "        self.window_size = window_size\n",
    "        self.max_models = max_models\n",
    "        self.window = InstanceWindow(window_size)\n",
    "        self.delay = delay\n",
    "        self.delay_counter = 0\n",
    "        \n",
    "    def partial_fit(self, X, y=None, classes=None):\n",
    "        # TODO \n",
    "        # if not initialized ...\n",
    "            # Setup \n",
    "        # N.B.: The 'classes' option is not important for this classifier\n",
    "        # HINT: You can build a decision tree model on a set of data like this:\n",
    "        #       h = DecisionTreeClassifier()\n",
    "        #       h.fit(X_batch,y_batch)\n",
    "        #       self.H.append(h) # <-- and append it to the ensemble\n",
    "        \n",
    "        \n",
    "        # self.h = DecisionTreeClassifier()\n",
    "        print(\"enter fit\")\n",
    "        N, D = X.shape;\n",
    "        \n",
    "        for i in range(N):\n",
    "            \n",
    "            self.window.add_element(np.asarray([X[i]]), np.asarray([[y[i]]]))\n",
    "            self.counter += 1\n",
    "            if (self.h) is None:\n",
    "                self.h = HMM()\n",
    "\n",
    "            # fit the new model if window_size is sufficient\n",
    "            if self.counter == self.window_size:\n",
    "\n",
    "                X_iter = self.window.get_attributes_matrix()\n",
    "                y_iter = self.window.get_targets_matrix()\n",
    "                self.h.fit(X_iter, y_iter)\n",
    "                self.counter=0\n",
    "                \n",
    "                # remove the most ancient model if the model_size is over than max_models\n",
    "                if(len(self.H) == self.max_models):\n",
    "                    self.H.pop(0)\n",
    "                self.H.append(self.h)\n",
    "        print(\"pass fit\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO \n",
    "        print(\"enter predict\")\n",
    "        print(\"len H is:\", len(self.H))\n",
    "        N, D = X.shape\n",
    "        res = zeros(len(self.H)) \n",
    "\n",
    "        for i in range(len(res)):\n",
    "            res[i] = self.H[i].predict(X)\n",
    "        print(\"pass predict\")\n",
    "\n",
    "        # You also need to change this line to return your prediction instead of 0s:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "from skmultiflow.evaluation.evaluate_prequential import EvaluatePrequential\n",
    "from skmultiflow.data.file_stream import FileStream\n",
    "from skmultiflow.lazy.knn_adwin import KNNAdwin, KNN\n",
    "from sklearn.linear_model.stochastic_gradient import SGDClassifier\n",
    "from skmultiflow.utils.utils import *\n",
    "\n",
    "from skmultiflow.utils.data_structures import InstanceWindow\n",
    "\n",
    "import matplotlib as plt\n",
    "\n",
    "stream = FileStream(\"words_int.csv\", n_targets=1, target_idx=-1)\n",
    "# 2. Prepare for use\n",
    "stream.prepare_for_use()\n",
    "# 2. Instantiate the HoeffdingTree classifier\n",
    "h = [\n",
    "        HMM(),\n",
    "        BatchClassifier()\n",
    "     ]\n",
    "\n",
    "# 3. Setup the evaluator\n",
    "\n",
    "evaluator = EvaluatePrequential(pretrain_size=1000, max_samples=2000, show_plot=True, \n",
    "                                metrics=['accuracy', 'kappa'], output_file='result.csv', \n",
    "                                batch_size=1)\n",
    "# 4. Run\n",
    "evaluator.evaluate(stream=stream, model=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
